Transformer from Scratch: English-Polish Translation ðŸ¤–
Custom-built transformer to deeply understand the architecture described in paper "Attention is All You Need". Every componentâ€”from multi-head attention mechanisms to positional encodingsâ€”has been implemented without relying on pre-built transformer libraries.

Dataset ðŸ’¾
The model trains on a bilingual corpus from Tatoeba, processed from TMX (Translation Memory eXchange) format. The dataset contains parallel English-Polish sentence pairs extracted from community-contributed translations.

This implementation serves as both a functional translation system and an educational deep-dive into transformer architecture fundamentals.